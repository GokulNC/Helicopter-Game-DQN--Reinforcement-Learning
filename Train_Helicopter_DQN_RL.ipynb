{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Helicopter-DQN-RL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GokulNC/Helicopter-Game-Reinforcement-Learning/blob/master/Train_Helicopter_DQN_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xanwbyMnvgJK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-8: The main runner code"
      ]
    },
    {
      "metadata": {
        "id": "lHQdMpHrvmmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random, pygame, signal, time\n",
        "import numpy as np\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "EPISODES = 15000\n",
        "starting_episode = 1\n",
        "updateTargetNetwork = 1000\n",
        "currentIteration = 0\n",
        "stop_flow = False\n",
        "\n",
        "\n",
        "def sigint_handler(signum, frame):\n",
        "    global stop_flow\n",
        "    print('Going to stop the flow after the current episode terminates...')\n",
        "    stop_flow = True\n",
        "    \n",
        "# To capture Ctrl-C events and stop gracefully\n",
        "# Source: https://pythonadventures.wordpress.com/2012/11/21/handle-ctrlc-in-your-script/\n",
        "signal.signal(signal.SIGINT, sigint_handler)\n",
        "\n",
        "# These dumps can be read by plot*.py and display the rewards/loss curve\n",
        "f = open('/tmp/dumps.txt', 'w')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tf.keras.backend.clear_session()\n",
        "    game = Pixelcopter(512, 512)\n",
        "    p = PLE(game, fps=20, force_fps=True, display_screen=False)\n",
        "    p.init()\n",
        "    state_size = 7 # TODO: Don't hardcode\n",
        "    action_size = 2 # Up and No-op\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    #agent.load(\"/tmp/heli-dqn-6000.h5\")\n",
        "    done = False\n",
        "    last_loss = 0\n",
        "\n",
        "    for e in range(starting_episode, EPISODES):\n",
        "        state = resetEnv(p)\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        while True:\n",
        "            currentIteration += 1\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = actInEnv(p, action)\n",
        "            reward = reward if not done else -10\n",
        "            total_reward += reward\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.3}\"\n",
        "                      .format(e, EPISODES, total_reward, agent.epsilon))\n",
        "                f.write(\"{},{},{}\\n\".format(e, total_reward, last_loss))\n",
        "                break\n",
        "            if len(agent.memory) > agent.batch_size*4:\n",
        "                last_loss = agent.replay_batch_gpu_optimized()\n",
        "                if currentIteration % updateTargetNetwork == 0:\n",
        "                    agent.target_train()\n",
        "            \n",
        "        if stop_flow:\n",
        "            break\n",
        "        if e % 500 == 0:\n",
        "            agent.save(\"/tmp/heli-dqn-{}.h5\".format(e))\n",
        "            print(\"Saved checkpoint!\")\n",
        "            # Decrease LR\n",
        "            K.set_value(agent.model.optimizer.lr, agent.learning_rate/pow(1.1, e/500))\n",
        "          \n",
        "f.close()\n",
        "agent.save(\"/tmp/heli-dqn-final.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lGe3_nBGwS_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-7: Gym-like Wrappers to PLE"
      ]
    },
    {
      "metadata": {
        "id": "txst-J0OwXw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pygame.constants import K_w, K_s\n",
        "\n",
        "def resetEnv(ple_env):\n",
        "    ple_env.reset_game()\n",
        "    return getCurrentState(ple_env)\n",
        "\n",
        "def getCurrentState(ple_env):\n",
        "    state_dict = ple_env.getGameState()\n",
        "    state = [state_dict[i] for i in state_dict]\n",
        "    return np.reshape(state, [1, len(state)])\n",
        "\n",
        "action_map = [K_w, K_s]\n",
        "def actInEnv(ple_env, action_num):\n",
        "    reward = ple_env.act(action_map[action_num])\n",
        "    state = getCurrentState(ple_env)\n",
        "    done = ple_env.game_over()\n",
        "    action = action_map[action_num]\n",
        "    return state, reward, done, action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2XDnKB-AujQy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-6: The below code contains the **DQN Agent** class. ([Inspiration](https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c))"
      ]
    },
    {
      "metadata": {
        "id": "GcjidH4ju4-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = 64\n",
        "        self.memory = deque(maxlen=20000)\n",
        "        self.gamma = 0.97    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.99995\n",
        "        self.learning_rate = 0.0004\n",
        "        self.tau = .125\n",
        "        self.train_on_TPU = False #Won't work if True\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        if self.train_on_TPU:\n",
        "            self.cpu_model = self.target_model.sync_to_cpu()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(48, activation='relu'))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dense(8, activation='relu'))\n",
        "        model.add(Dense(self.action_size)) # default is linear activation\n",
        "        if self.train_on_TPU:\n",
        "            model = tf.contrib.tpu.keras_to_tpu_model(model,\n",
        "                strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "                tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "            ))\n",
        "            model.compile(loss='mean_squared_error',\n",
        "                      optimizer=tf.train.AdamOptimizer(learning_rate=self.learning_rate))\n",
        "        else:\n",
        "            model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append([state, action, reward, next_state, done])\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice([0, 1], p=[0.25, 0.75]) #random.randrange(self.action_size)\n",
        "#         if self.train_on_TPU: # Sorry for this hack, Google\n",
        "#             state = state.repeat(self.batch_size, axis=0)\n",
        "        act_values = self.model.predict(state) if not self.train_on_TPU else self.cpu_model.predict(state) \n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self):\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.target_model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def replay_batch(self):\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states, targets_f = [], []\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.target_model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target \n",
        "            # Filtering out states and targets for training\n",
        "            states.append(state[0])\n",
        "            targets_f.append(target_f[0])\n",
        "        \n",
        "        history = self.model.fit(np.array(states), np.array(targets_f), batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "        # Keeping track of loss\n",
        "        loss = history.history['loss'][0]\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        return loss\n",
        "\n",
        "    def replay_batch_gpu_optimized(self):\n",
        "        # Check the above reference implementation for correspondance\n",
        "        minibatch = np.array(random.sample(self.memory, self.batch_size))\n",
        "        state = np.array(minibatch[:, 0].tolist()).squeeze()\n",
        "        action = minibatch[:, 1]\n",
        "        target = minibatch[:, 2]\n",
        "        next_state = np.array(minibatch[:, 3].tolist()).squeeze()\n",
        "        done = np.array(minibatch[:, 4], dtype=bool)\n",
        "        Q_next_max = self.gamma * np.amax(self.target_model.predict(next_state, batch_size=self.batch_size), axis=1)\n",
        "        target = target + (Q_next_max * np.invert(done))\n",
        "        target_f = self.model.predict(state, batch_size=self.batch_size)\n",
        "        target_f[range(self.batch_size), action.tolist()] = target\n",
        "        \n",
        "        history = self.model.fit(state, target_f, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "        # Keeping track of loss\n",
        "        loss = history.history['loss'][0]\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        return loss\n",
        "      \n",
        "    def target_train(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        # Any better way than below? ;(\n",
        "        for i in range(len(target_weights)):\n",
        "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
        "        self.target_model.set_weights(target_weights)\n",
        "        if self.train_on_TPU: # I hope this is not a costly operation\n",
        "            self.cpu_model = self.target_model.sync_to_cpu()\n",
        "    \n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "        self.target_model.load_weights(name)\n",
        "        if self.train_on_TPU:\n",
        "            self.cpu_model.load_weights(name)            \n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KkejsrFbq7iP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-5: Below is the helicopter game's PLE env modified by me ([Source](https://github.com/ntasfi/PyGame-Learning-Environment/blob/master/ple/games/pixelcopter.py))"
      ]
    },
    {
      "metadata": {
        "id": "ZDcF7RgprHZG",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "4afa8456-14f3-4aa3-a882-1e4183afeb8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "img_dir = '/tmp/assets/'\n",
        "if not os.path.exists(img_dir):\n",
        "    os.makedirs(img_dir)\n",
        "!wget -P /tmp/assets/ https://github.com/code-master5/SaveTheHeli/raw/master/heli2.png\n",
        "  \n",
        "\n",
        "import math\n",
        "import sys, os\n",
        "\n",
        "import pygame\n",
        "from pygame.constants import K_w, K_s\n",
        "\n",
        "\n",
        "class Block(pygame.sprite.Sprite):\n",
        "\n",
        "    def __init__(self, pos_init, speed, SCREEN_WIDTH, SCREEN_HEIGHT):\n",
        "        pygame.sprite.Sprite.__init__(self)\n",
        "\n",
        "        self.pos = vec2d(pos_init)\n",
        "\n",
        "        self.width = int(SCREEN_WIDTH * 0.07)\n",
        "        self.height = int(SCREEN_HEIGHT * 0.1)\n",
        "        self.speed = speed\n",
        "\n",
        "        self.SCREEN_WIDTH = SCREEN_WIDTH\n",
        "        self.SCREEN_HEIGHT = SCREEN_HEIGHT\n",
        "\n",
        "        image = pygame.Surface((self.width, self.height))\n",
        "        image.fill((0, 0, 0, 0))\n",
        "        image.set_colorkey((0, 0, 0))\n",
        "\n",
        "        pygame.draw.rect(\n",
        "            image,\n",
        "            (120, 240, 80),\n",
        "            (0, 0, self.width, self.height),\n",
        "            0\n",
        "        )\n",
        "\n",
        "        self.image = image\n",
        "        self.rect = self.image.get_rect()\n",
        "        self.rect.center = pos_init\n",
        "\n",
        "    def update(self, dt):\n",
        "        self.pos.x -= self.speed * dt\n",
        "\n",
        "        self.rect.center = (self.pos.x, self.pos.y)\n",
        "\n",
        "\n",
        "class HelicopterPlayer(pygame.sprite.Sprite):\n",
        "\n",
        "    def __init__(self, speed, SCREEN_WIDTH, SCREEN_HEIGHT):\n",
        "        pygame.sprite.Sprite.__init__(self)\n",
        "\n",
        "        pos_init = (int(SCREEN_WIDTH * 0.35), SCREEN_HEIGHT / 2)\n",
        "        self.pos = vec2d(pos_init)\n",
        "        self.speed = speed\n",
        "        self.climb_speed = speed * -0.875  # -0.0175\n",
        "        self.fall_speed = speed * 0.09  # 0.0019\n",
        "        self.momentum = 0\n",
        "\n",
        "        self.width = SCREEN_WIDTH * 0.1\n",
        "        self.height = SCREEN_HEIGHT * 0.05\n",
        "        \n",
        "        heli_sprite_path = img_dir+\"heli2.png\"\n",
        "        self.image = pygame.image.load(heli_sprite_path).convert_alpha()\n",
        "        self.image = pygame.transform.scale(self.image, (int(self.width), int(self.height)))\n",
        "        #image = pygame.Surface((self.width, self.height))\n",
        "        #image.fill((0, 0, 0, 0))\n",
        "        #image.set_colorkey((0, 0, 0))\n",
        "\n",
        "        #pygame.draw.rect(\n",
        "        #    image,\n",
        "        #    (255, 255, 255),\n",
        "        #    (0, 0, self.width, self.height),\n",
        "        #    0\n",
        "        #)\n",
        "\n",
        "        #self.image = image\n",
        "        self.rect = self.image.get_rect()\n",
        "        self.rect.center = pos_init\n",
        "\n",
        "    def update(self, is_climbing, dt):\n",
        "        self.momentum += (self.climb_speed if is_climbing else self.fall_speed) * dt\n",
        "        self.momentum *= 0.99\n",
        "        self.pos.y += self.momentum\n",
        "\n",
        "        self.rect.center = (self.pos.x, self.pos.y)\n",
        "\n",
        "\n",
        "class Terrain(pygame.sprite.Sprite):\n",
        "\n",
        "    def __init__(self, pos_init, speed, SCREEN_WIDTH, SCREEN_HEIGHT):\n",
        "        pygame.sprite.Sprite.__init__(self)\n",
        "\n",
        "        self.pos = vec2d(pos_init)\n",
        "        self.speed = speed\n",
        "        self.width = int(SCREEN_WIDTH * 0.1)\n",
        "\n",
        "        image = pygame.Surface((self.width, SCREEN_HEIGHT * 1.5))\n",
        "        image.fill((0, 0, 0, 0))\n",
        "        image.set_colorkey((0, 0, 0))\n",
        "\n",
        "        color = (120, 240, 80)\n",
        "\n",
        "        # top rect\n",
        "        pygame.draw.rect(\n",
        "            image,\n",
        "            color,\n",
        "            (0, 0, self.width, SCREEN_HEIGHT * 0.5),\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # bot rect\n",
        "        pygame.draw.rect(\n",
        "            image,\n",
        "            color,\n",
        "            (0, SCREEN_HEIGHT * 1.05, self.width, SCREEN_HEIGHT * 0.5),\n",
        "            0\n",
        "        )\n",
        "\n",
        "        self.image = image\n",
        "        self.rect = self.image.get_rect()\n",
        "        self.rect.center = pos_init\n",
        "\n",
        "    def update(self, dt):\n",
        "        self.pos.x -= self.speed * dt\n",
        "        self.rect.center = (self.pos.x, self.pos.y)\n",
        "\n",
        "\n",
        "class Pixelcopter(PyGameWrapper):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    width : int\n",
        "        Screen width.\n",
        "\n",
        "    height : int\n",
        "        Screen height, recommended to be same dimension as width.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, width=48, height=48):\n",
        "        actions = {\n",
        "            \"up\": K_w\n",
        "        }\n",
        "\n",
        "        PyGameWrapper.__init__(self, width, height, actions=actions)\n",
        "\n",
        "        self.is_climbing = False\n",
        "        self.speed = 0.0004 * width\n",
        "        \n",
        "        #self._dir_ = os.path.dirname(os.path.abspath(__file__))\n",
        "        #self._asset_dir = os.path.join(self._dir_, \"assets/\")\n",
        "\n",
        "    def _handle_player_events(self):\n",
        "        self.is_climbing = False\n",
        "\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                pygame.quit()\n",
        "                sys.exit()\n",
        "\n",
        "            if event.type == pygame.KEYDOWN:\n",
        "                key = event.key\n",
        "                if key == self.actions['up']:\n",
        "                    self.is_climbing = True\n",
        "\n",
        "    def getGameState(self):\n",
        "        \"\"\"\n",
        "        Gets a non-visual state representation of the game.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        dict\n",
        "            * player y position.\n",
        "            * player velocity.\n",
        "            * player distance to floor.\n",
        "            * player distance to ceiling.\n",
        "            * next block x distance to player.\n",
        "            * next blocks top y location,\n",
        "            * next blocks bottom y location.\n",
        "\n",
        "            See code for structure.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        min_dist = 999\n",
        "        min_block = None\n",
        "        for b in self.block_group:  # Groups do not return in order\n",
        "            dist_to = b.pos.x - self.player.pos.x\n",
        "            if dist_to > 0 and dist_to < min_dist:\n",
        "                min_block = b\n",
        "                min_dist = dist_to\n",
        "\n",
        "        current_terrain = pygame.sprite.spritecollide(\n",
        "            self.player, self.terrain_group, False)[0]\n",
        "        state = {\n",
        "            \"player_y\": self.player.pos.y,\n",
        "            \"player_vel\": self.player.momentum,\n",
        "            \"player_dist_to_ceil\": self.player.pos.y - (current_terrain.pos.y - self.height * 0.25),\n",
        "            \"player_dist_to_floor\": (current_terrain.pos.y + self.height * 0.25) - self.player.pos.y,\n",
        "            \"next_gate_dist_to_player\": min_dist,\n",
        "            \"next_gate_block_top\": min_block.pos.y,\n",
        "            \"next_gate_block_bottom\": min_block.pos.y + min_block.height\n",
        "        }\n",
        "\n",
        "        return state\n",
        "\n",
        "    def getScreenDims(self):\n",
        "        return self.screen_dim\n",
        "\n",
        "    def getActions(self):\n",
        "        return self.actions.values()\n",
        "\n",
        "    def getScore(self):\n",
        "        return self.score\n",
        "\n",
        "    def game_over(self):\n",
        "        return self.lives <= 0.0\n",
        "\n",
        "    def init(self):\n",
        "        self.score = 0.0\n",
        "        self.lives = 1.0\n",
        "\n",
        "        self.player = HelicopterPlayer(\n",
        "            self.speed,\n",
        "            self.width,\n",
        "            self.height\n",
        "        )\n",
        "\n",
        "        self.player_group = pygame.sprite.Group()\n",
        "        self.player_group.add(self.player)\n",
        "\n",
        "        self.block_group = pygame.sprite.Group()\n",
        "        self._add_blocks()\n",
        "\n",
        "        self.terrain_group = pygame.sprite.Group()\n",
        "        self._add_terrain(0, self.width * 4)\n",
        "\n",
        "    def _add_terrain(self, start, end):\n",
        "        w = int(self.width * 0.1)\n",
        "        # each block takes up 10 units.\n",
        "        steps = range(start + int(w / 2), end + int(w / 2), w)\n",
        "        y_jitter = []\n",
        "\n",
        "        freq = 4.5 / self.width + self.rng.uniform(-0.01, 0.01)\n",
        "        for step in steps:\n",
        "            jitter = (self.height * 0.125) * \\\n",
        "                math.sin(freq * step + self.rng.uniform(0.0, 0.5))\n",
        "            y_jitter.append(jitter)\n",
        "\n",
        "        y_pos = [int((self.height / 2.0) + y_jit) for y_jit in y_jitter]\n",
        "\n",
        "        for i in range(0, len(steps)):\n",
        "            self.terrain_group.add(Terrain(\n",
        "                (steps[i], y_pos[i]),\n",
        "                self.speed,\n",
        "                self.width,\n",
        "                self.height\n",
        "            )\n",
        "            )\n",
        "\n",
        "    def _add_blocks(self):\n",
        "        x_pos = self.rng.randint(self.width, int(self.width * 1.5))\n",
        "        y_pos = self.rng.randint(\n",
        "            int(self.height * 0.25),\n",
        "            int(self.height * 0.75)\n",
        "        )\n",
        "        self.block_group.add(\n",
        "            Block(\n",
        "                (x_pos, y_pos),\n",
        "                self.speed,\n",
        "                self.width,\n",
        "                self.height\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.init()\n",
        "\n",
        "    def step(self, dt):\n",
        "\n",
        "        self.screen.fill((0, 0, 0))\n",
        "        self._handle_player_events()\n",
        "\n",
        "        self.score += self.rewards[\"tick\"]\n",
        "\n",
        "        self.player.update(self.is_climbing, dt)\n",
        "        self.block_group.update(dt)\n",
        "        self.terrain_group.update(dt)\n",
        "\n",
        "        hits = pygame.sprite.spritecollide(\n",
        "            self.player, self.block_group, False)\n",
        "        for creep in hits:\n",
        "            self.lives -= 1\n",
        "\n",
        "        hits = pygame.sprite.spritecollide(\n",
        "            self.player, self.terrain_group, False)\n",
        "        for t in hits:\n",
        "            if self.player.pos.y - self.player.height <= t.pos.y - self.height * 0.25:\n",
        "                self.lives -= 1\n",
        "\n",
        "            if self.player.pos.y >= t.pos.y + self.height * 0.25:\n",
        "                self.lives -= 1\n",
        "\n",
        "        for b in self.block_group:\n",
        "            if b.pos.x <= self.player.pos.x and len(self.block_group) == 1:\n",
        "                self.score += self.rewards[\"positive\"]\n",
        "                self._add_blocks()\n",
        "\n",
        "            if b.pos.x <= -b.width:\n",
        "                b.kill()\n",
        "\n",
        "        for t in self.terrain_group:\n",
        "            if t.pos.x <= -t.width:\n",
        "                self.score += self.rewards[\"positive\"]\n",
        "                t.kill()\n",
        "\n",
        "        if self.player.pos.y < self.height * 0.125:  # its above\n",
        "            self.lives -= 1\n",
        "\n",
        "        if self.player.pos.y > self.height * 0.875:  # its below the lowest possible block\n",
        "            self.lives -= 1\n",
        "\n",
        "        if len(self.terrain_group) <= (\n",
        "                10 + 3):  # 10% per terrain, offset of ~2 with 1 extra\n",
        "            self._add_terrain(self.width, self.width * 5)\n",
        "\n",
        "        if self.lives <= 0.0:\n",
        "            self.score += self.rewards[\"loss\"]\n",
        "\n",
        "        self.player_group.draw(self.screen)\n",
        "        self.block_group.draw(self.screen)\n",
        "        self.terrain_group.draw(self.screen)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import numpy as np\n",
        "\n",
        "#     pygame.init()\n",
        "#     game = Pixelcopter(width=256, height=256)\n",
        "#     game.screen = pygame.display.set_mode(game.getScreenDims(), 0, 32)\n",
        "#     game.clock = pygame.time.Clock()\n",
        "#     game.rng = np.random.RandomState(24)\n",
        "#     game.init()\n",
        "\n",
        "#     while True:\n",
        "#         if game.game_over():\n",
        "#             game.reset()\n",
        "#         dt = game.clock.tick_busy_loop(30)\n",
        "#         game.step(dt)\n",
        "#         pygame.display.update()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-01 16:04:18--  https://github.com/code-master5/SaveTheHeli/raw/master/heli2.png\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/code-master5/SaveTheHeli/master/heli2.png [following]\n",
            "--2019-02-01 16:04:19--  https://raw.githubusercontent.com/code-master5/SaveTheHeli/master/heli2.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7240 (7.1K) [image/png]\n",
            "Saving to: ‘/tmp/assets/heli2.png’\n",
            "\n",
            "heli2.png           100%[===================>]   7.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-01 16:04:20 (116 MB/s) - ‘/tmp/assets/heli2.png’ saved [7240/7240]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vc0CdaDcp2hM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-4: Environment Wrapper for all games ([Source](https://github.com/ntasfi/PyGame-Learning-Environment/blob/master/ple/games/base/pygamewrapper.py))"
      ]
    },
    {
      "metadata": {
        "id": "izdUd7iupgoV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import pygame\n",
        "import numpy as np\n",
        "from pygame.constants import KEYDOWN, KEYUP, K_F15\n",
        "\n",
        "\n",
        "class PyGameWrapper(object):\n",
        "    \"\"\"PyGameWrapper  class\n",
        "\n",
        "    ple.games.base.PyGameWrapper(width, height, actions={})\n",
        "\n",
        "    This :class:`PyGameWrapper` class sets methods all games require. It should be subclassed when creating new games.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    width: int\n",
        "        The width of the game screen.\n",
        "\n",
        "    height: int\n",
        "        The height of the game screen.\n",
        "\n",
        "    actions: dict\n",
        "        Contains possible actions that the game responds too. The dict keys are used by the game, while the values are `pygame.constants` referring the keys.\n",
        "\n",
        "        Possible actions dict:\n",
        "\n",
        "        >>> from pygame.constants import K_w, K_s\n",
        "        >>> actions = {\n",
        "        >>>     \"up\": K_w,\n",
        "        >>>     \"down\": K_s\n",
        "        >>> }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, width, height, actions={}):\n",
        "\n",
        "        # Required fields\n",
        "        self.actions = actions  # holds actions\n",
        "\n",
        "        self.score = 0.0  # required.\n",
        "        self.lives = 0  # required. Can be 0 or -1 if not required.\n",
        "        self.screen = None  # must be set to None\n",
        "        self.clock = None  # must be set to None\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.screen_dim = (width, height)  # width and height\n",
        "        self.allowed_fps = None  # fps that the game is allowed to run at.\n",
        "        self.NOOP = K_F15  # the noop key\n",
        "        self.rng = None\n",
        "\n",
        "        self.rewards = {\n",
        "            \"positive\": 1.0,\n",
        "            \"negative\": -1.0,\n",
        "            \"tick\": 0,\n",
        "            \"loss\": -5.0,\n",
        "            \"win\": 5.0\n",
        "        }\n",
        "\n",
        "    def _setup(self):\n",
        "        \"\"\"\n",
        "        Setups up the pygame env, the display and game clock.\n",
        "        \"\"\"\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode(self.getScreenDims(), 0, 32)\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    def _setAction(self, action, last_action):\n",
        "        \"\"\"\n",
        "        Pushes the action to the pygame event queue.\n",
        "        \"\"\"\n",
        "        if action is None:\n",
        "            action = self.NOOP\n",
        "\n",
        "        if last_action is None:\n",
        "            last_action = self.NOOP\n",
        "\n",
        "        kd = pygame.event.Event(KEYDOWN, {\"key\": action})\n",
        "        ku = pygame.event.Event(KEYUP, {\"key\": last_action})\n",
        "\n",
        "        pygame.event.post(kd)\n",
        "        pygame.event.post(ku)\n",
        "\n",
        "    def _draw_frame(self, draw_screen):\n",
        "        \"\"\"\n",
        "        Decides if the screen will be drawn too\n",
        "        \"\"\"\n",
        "\n",
        "        if draw_screen == True:\n",
        "            pygame.display.update()\n",
        "\n",
        "    def getScreenRGB(self):\n",
        "        \"\"\"\n",
        "        Returns the current game screen in RGB format.\n",
        "\n",
        "        Returns\n",
        "        --------\n",
        "        numpy uint8 array\n",
        "            Returns a numpy array with the shape (width, height, 3).\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return pygame.surfarray.array3d(\n",
        "            pygame.display.get_surface()).astype(np.uint8)\n",
        "\n",
        "    def tick(self, fps):\n",
        "        \"\"\"\n",
        "        This sleeps the game to ensure it runs at the desired fps.\n",
        "        \"\"\"\n",
        "        return self.clock.tick_busy_loop(fps)\n",
        "\n",
        "    def adjustRewards(self, rewards):\n",
        "        \"\"\"\n",
        "\n",
        "        Adjusts the rewards the game gives the agent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rewards : dict\n",
        "            A dictonary of reward events to float rewards. Only updates if key matches those specificed in the init function.\n",
        "\n",
        "        \"\"\"\n",
        "        for key in rewards.keys():\n",
        "            if key in self.rewards:\n",
        "                self.rewards[key] = rewards[key]\n",
        "\n",
        "    def setRNG(self, rng):\n",
        "        \"\"\"\n",
        "        Sets the rng for games.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.rng is None:\n",
        "            self.rng = rng\n",
        "\n",
        "    def getGameState(self):\n",
        "        \"\"\"\n",
        "        Gets a non-visual state representation of the game.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict or None\n",
        "            dict if the game supports it and None otherwise.\n",
        "\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def getScreenDims(self):\n",
        "        \"\"\"\n",
        "        Gets the screen dimensions of the game in tuple form.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of int\n",
        "            Returns tuple as follows (width, height).\n",
        "\n",
        "        \"\"\"\n",
        "        return self.screen_dim\n",
        "\n",
        "    def getActions(self):\n",
        "        \"\"\"\n",
        "        Gets the actions used within the game.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list of `pygame.constants`\n",
        "\n",
        "        \"\"\"\n",
        "        return self.actions.values()\n",
        "\n",
        "    def init(self):\n",
        "        \"\"\"\n",
        "        This is used to initialize the game, such reseting the score, lives, and player position.\n",
        "\n",
        "        This is game dependent.\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Please override this method\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Wraps the init() function, can be setup to reset certain poritions of the game only if needed.\n",
        "        \"\"\"\n",
        "        self.init()\n",
        "\n",
        "    def getScore(self):\n",
        "        \"\"\"\n",
        "        Return the current score of the game.\n",
        "\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The current reward the agent has received since the last init() or reset() call.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Please override this method\")\n",
        "\n",
        "    def game_over(self):\n",
        "        \"\"\"\n",
        "        Gets the status of the game, returns True if game has hit a terminal state. False otherwise.\n",
        "\n",
        "        This is game dependent.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        bool\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Please override this method\")\n",
        "\n",
        "    def step(self, dt):\n",
        "        \"\"\"\n",
        "        This method steps the game forward one step in time equal to the dt parameter. The game does not run unless this method is called.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dt : integer\n",
        "            This is the amount of time elapsed since the last frame in milliseconds.\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Please override this method\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5fc1b-LqLu-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-3: Environment Interface ([Source](https://github.com/ntasfi/PyGame-Learning-Environment/blob/master/ple/ple.py))"
      ]
    },
    {
      "metadata": {
        "id": "oDWvJZzLpznn",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ed532e8-6b87-47d0-e3ad-dcdd9984c397"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "from PIL import Image  # pillow\n",
        "import sys\n",
        "import pygame\n",
        "\n",
        "class PLE(object):\n",
        "    \"\"\"\n",
        "    ple.PLE(\n",
        "        game, fps=30,\n",
        "        frame_skip=1, num_steps=1,\n",
        "        reward_values={}, force_fps=True,\n",
        "        display_screen=False, add_noop_action=True,\n",
        "        NOOP=K_F15, state_preprocessor=None,\n",
        "        rng=24\n",
        "    )\n",
        "\n",
        "    Main wrapper that interacts with games.\n",
        "    Provides a similar interface to Arcade Learning Environment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    game: Class from ple.games.base\n",
        "        The game the PLE environment manipulates and maintains.\n",
        "\n",
        "    fps: int (default: 30)\n",
        "        The desired frames per second we want to run our game at.\n",
        "            Typical settings are 30 and 60 fps.\n",
        "\n",
        "    frame_skip: int (default: 1)\n",
        "        The number of times we skip getting observations while\n",
        "        repeat an action.\n",
        "\n",
        "    num_steps: int (default: 1)\n",
        "        The number of times we repeat an action.\n",
        "\n",
        "    reward_values: dict\n",
        "        This contains the rewards we wish to set give our agent based on\n",
        "        different actions in game. The current defaults are as follows:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            rewards = {\n",
        "                \"positive\": 1.0,\n",
        "                \"negative\": -1.0,\n",
        "                \"tick\": 0.0,\n",
        "                \"loss\": -5.0,\n",
        "                \"win\": 5.0\n",
        "            }\n",
        "\n",
        "        Tick is given to the agent at each game step. You can selectively\n",
        "        adjust the rewards by passing a dictonary with the key you want to\n",
        "        change. Eg. If we want to adjust the negative reward and the tick\n",
        "        reward we would pass in the following:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            rewards = {\n",
        "                \"negative\": -2.0,\n",
        "                \"tick\": -0.01\n",
        "            }\n",
        "\n",
        "        Keep in mind that the tick is applied at each frame. If the game is\n",
        "        running at 60fps the agent will get a reward of 60*tick.\n",
        "\n",
        "    force_fps: bool (default: True)\n",
        "        If False PLE delays between game.step() calls to ensure the fps is\n",
        "        specified. If not PLE passes an elapsed time delta to ensure the\n",
        "        game steps by an amount of time consistent with the specified fps.\n",
        "        This is usally set to True as it allows the game to run as fast as\n",
        "        possible which speeds up training.\n",
        "\n",
        "    display_screen: bool (default: False)\n",
        "        If we draw updates to the screen. Disabling this speeds up\n",
        "        interation speed. This can be toggled to True during testing phases\n",
        "        so you can observe the agents progress.\n",
        "\n",
        "    add_noop_action: bool (default: True)\n",
        "        This inserts the NOOP action specified as a valid move the agent\n",
        "        can make.\n",
        "\n",
        "    state_preprocessor: python function (default: None)\n",
        "        Python function which takes a dict representing game state and\n",
        "        returns a numpy array.\n",
        "\n",
        "    rng: numpy.random.RandomState, int, array_like or None. (default: 24)\n",
        "        Number generator which is used by PLE and the games.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 game, fps=30, frame_skip=1, num_steps=1,\n",
        "                 reward_values={}, force_fps=True, display_screen=False,\n",
        "                 add_noop_action=True, state_preprocessor=None, rng=24):\n",
        "\n",
        "        self.game = game\n",
        "        self.fps = fps\n",
        "        self.frame_skip = frame_skip\n",
        "        self.NOOP = None\n",
        "        self.num_steps = num_steps\n",
        "        self.force_fps = force_fps\n",
        "        self.display_screen = display_screen\n",
        "        self.add_noop_action = add_noop_action\n",
        "\n",
        "        self.last_action = []\n",
        "        self.action = []\n",
        "        self.previous_score = 0\n",
        "        self.frame_count = 0\n",
        "\n",
        "        # update the scores of games with values we pick\n",
        "        if reward_values:\n",
        "            self.game.adjustRewards(reward_values)\n",
        "\n",
        "\n",
        "        if isinstance(self.game, PyGameWrapper):\n",
        "            if isinstance(rng, np.random.RandomState):\n",
        "                self.rng = rng\n",
        "            else:\n",
        "                self.rng = np.random.RandomState(rng)\n",
        "\n",
        "            # some pygame games preload the images\n",
        "            # to speed resetting and inits up.\n",
        "            pygame.display.set_mode((1, 1), pygame.NOFRAME)\n",
        "        else:\n",
        "            # in order to use doom, install following https://github.com/openai/doom-py\n",
        "            from .games.base.doomwrapper import DoomWrapper\n",
        "            if isinstance(self.game, DoomWrapper):\n",
        "                self.rng = rng\n",
        "        \n",
        "        self.game.setRNG(self.rng)\n",
        "        self.init()\n",
        "\n",
        "        self.state_preprocessor = state_preprocessor\n",
        "        self.state_dim = None\n",
        "\n",
        "        if self.state_preprocessor is not None:\n",
        "            self.state_dim = self.game.getGameState()\n",
        "\n",
        "            if self.state_dim is None:\n",
        "                raise ValueError(\n",
        "                    \"Asked to return non-visual state on game that does not support it!\")\n",
        "            else:\n",
        "                self.state_dim = self.state_preprocessor(self.state_dim).shape\n",
        "\n",
        "        if game.allowed_fps is not None and self.fps != game.allowed_fps:\n",
        "            raise ValueError(\"Game requires %dfps, was given %d.\" %\n",
        "                             (game.allowed_fps, game.allowed_fps))\n",
        "\n",
        "    def _tick(self):\n",
        "        \"\"\"\n",
        "        Calculates the elapsed time between frames or ticks.\n",
        "        \"\"\"\n",
        "        if self.force_fps:\n",
        "            return 1000.0 / self.fps\n",
        "        else:\n",
        "            return self.game.tick(self.fps)\n",
        "\n",
        "    def init(self):\n",
        "        \"\"\"\n",
        "        Initializes the game. This depends on the game and could include\n",
        "        doing things such as setting up the display, clock etc.\n",
        "\n",
        "        This method should be explicitly called.\n",
        "        \"\"\"\n",
        "        self.game._setup()\n",
        "        self.game.init() #this is the games setup/init\n",
        "\n",
        "    def getActionSet(self):\n",
        "        \"\"\"\n",
        "        Gets the actions the game supports. Optionally inserts the NOOP\n",
        "        action if PLE has add_noop_action set to True.\n",
        "\n",
        "        Returns\n",
        "        --------\n",
        "\n",
        "        list of pygame.constants\n",
        "            The agent can simply select the index of the action\n",
        "            to perform.\n",
        "\n",
        "        \"\"\"\n",
        "        actions = self.game.actions\n",
        "\n",
        "        if (sys.version_info > (3, 0)): #python ver. 3\n",
        "            if isinstance(actions, dict) or isinstance(actions, dict_values):\n",
        "                actions = actions.values()\n",
        "        else:\n",
        "            if isinstance(actions, dict):\n",
        "                actions = actions.values()\n",
        "\n",
        "        actions = list(actions) #.values()\n",
        "        #print (actions)\n",
        "        #assert isinstance(actions, list), \"actions is not a list\"\n",
        "\n",
        "        if self.add_noop_action:\n",
        "            actions.append(self.NOOP)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def getFrameNumber(self):\n",
        "        \"\"\"\n",
        "        Gets the current number of frames the agent has seen\n",
        "        since PLE was initialized.\n",
        "\n",
        "        Returns\n",
        "        --------\n",
        "\n",
        "        int\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.frame_count\n",
        "\n",
        "    def game_over(self):\n",
        "        \"\"\"\n",
        "        Returns True if the game has reached a terminal state and\n",
        "        False otherwise.\n",
        "\n",
        "        This state is game dependent.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        bool\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.game.game_over()\n",
        "\n",
        "    def score(self):\n",
        "        \"\"\"\n",
        "        Gets the score the agent currently has in game.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        int\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.game.getScore()\n",
        "\n",
        "    def lives(self):\n",
        "        \"\"\"\n",
        "        Gets the number of lives the agent has left. Not all games have\n",
        "        the concept of lives.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        int\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.game.lives\n",
        "\n",
        "    def reset_game(self):\n",
        "        \"\"\"\n",
        "        Performs a reset of the games to a clean initial state.\n",
        "        \"\"\"\n",
        "        self.last_action = []\n",
        "        self.action = []\n",
        "        self.previous_score = 0.0\n",
        "        self.game.reset()\n",
        "\n",
        "    def getScreenRGB(self):\n",
        "        \"\"\"\n",
        "        Gets the current game screen in RGB format.\n",
        "\n",
        "        Returns\n",
        "        --------\n",
        "        numpy uint8 array\n",
        "            Returns a numpy array with the shape (width, height, 3).\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.game.getScreenRGB()\n",
        "\n",
        "    def getScreenGrayscale(self):\n",
        "        \"\"\"\n",
        "        Gets the current game screen in Grayscale format. Converts from RGB using relative lumiance.\n",
        "\n",
        "        Returns\n",
        "        --------\n",
        "        numpy uint8 array\n",
        "                Returns a numpy array with the shape (width, height).\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        frame = self.getScreenRGB()\n",
        "        frame = 0.21 * frame[:, :, 0] + 0.72 * \\\n",
        "            frame[:, :, 1] + 0.07 * frame[:, :, 2]\n",
        "        frame = np.round(frame).astype(np.uint8)\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def saveScreen(self, filename):\n",
        "        \"\"\"\n",
        "        Saves the current screen to png file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        filename : string\n",
        "            The path with filename to where we want the image saved.\n",
        "\n",
        "        \"\"\"\n",
        "        frame = Image.fromarray(self.getScreenRGB())\n",
        "        frame.save(filename)\n",
        "\n",
        "    def getScreenDims(self):\n",
        "        \"\"\"\n",
        "        Gets the games screen dimensions.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        tuple of int\n",
        "            Returns a tuple of the following format (screen_width, screen_height).\n",
        "        \"\"\"\n",
        "        return self.game.getScreenDims()\n",
        "\n",
        "    def getGameStateDims(self):\n",
        "        \"\"\"\n",
        "        Gets the games non-visual state dimensions.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        tuple of int or None\n",
        "            Returns a tuple of the state vectors shape or None if the game does not support it.\n",
        "        \"\"\"\n",
        "        return self.state_dim\n",
        "\n",
        "    def getGameState(self):\n",
        "        \"\"\"\n",
        "        Gets a non-visual state representation of the game.\n",
        "\n",
        "        This can include items such as player position, velocity, ball location and velocity etc.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        dict or None\n",
        "            It returns a dict of game information. This greatly depends on the game in question and must be referenced against each game.\n",
        "            If no state is available or supported None will be returned back.\n",
        "\n",
        "        \"\"\"\n",
        "        state = self.game.getGameState()\n",
        "        if state is not None:\n",
        "            if self.state_preprocessor is not None:\n",
        "                return self.state_preprocessor(state)\n",
        "            return state\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Was asked to return state vector for game that does not support it!\")\n",
        "\n",
        "    def act(self, action):\n",
        "        \"\"\"\n",
        "        Perform an action on the game. We lockstep frames with actions. If act is not called the game will not run.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "\n",
        "        action : int\n",
        "            The index of the action we wish to perform. The index usually corresponds to the index item returned by getActionSet().\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "\n",
        "        int\n",
        "            Returns the reward that the agent has accumlated while performing the action.\n",
        "\n",
        "        \"\"\"\n",
        "        return sum(self._oneStepAct(action) for i in range(self.frame_skip))\n",
        "\n",
        "    def _draw_frame(self):\n",
        "        \"\"\"\n",
        "        Decides if the screen will be drawn too\n",
        "        \"\"\"\n",
        "\n",
        "        self.game._draw_frame(self.display_screen)\n",
        "\n",
        "    def _oneStepAct(self, action):\n",
        "        \"\"\"\n",
        "        Performs an action on the game. Checks if the game is over or if the provided action is valid based on the allowed action set.\n",
        "        \"\"\"\n",
        "        if self.game_over():\n",
        "            return 0.0\n",
        "\n",
        "        if action not in self.getActionSet():\n",
        "            action = self.NOOP\n",
        "\n",
        "        self._setAction(action)\n",
        "        for i in range(self.num_steps):\n",
        "            time_elapsed = self._tick()\n",
        "            self.game.step(time_elapsed)\n",
        "            self._draw_frame()\n",
        "\n",
        "        self.frame_count += self.num_steps\n",
        "\n",
        "        return self._getReward()\n",
        "\n",
        "    def _setAction(self, action):\n",
        "        \"\"\"\n",
        "            Instructs the game to perform an action if its not a NOOP\n",
        "        \"\"\"\n",
        "\n",
        "        if action is not None:\n",
        "            self.game._setAction(action, self.last_action)\n",
        "\n",
        "        self.last_action = action\n",
        "\n",
        "    def _getReward(self):\n",
        "        \"\"\"\n",
        "        Returns the reward the agent has gained as the difference between the last action and the current one.\n",
        "        \"\"\"\n",
        "        reward = self.game.getScore() - self.previous_score\n",
        "        self.previous_score = self.game.getScore()\n",
        "\n",
        "        return reward"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.4\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LLxSosk2rS6A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-2: Utils: ([Source](https://github.com/ntasfi/PyGame-Learning-Environment/blob/master/ple/games/utils/vec2d.py))"
      ]
    },
    {
      "metadata": {
        "id": "cSAS3MW-rT_T",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import math\n",
        "\n",
        "\n",
        "class vec2d():\n",
        "\n",
        "    def __init__(self, pos):\n",
        "        self.x = pos[0]\n",
        "        self.y = pos[1]\n",
        "\n",
        "    def __add__(self, o):\n",
        "        x = self.x + o.x\n",
        "        y = self.y + o.y\n",
        "\n",
        "        return vec2d((x, y))\n",
        "\n",
        "    def __eq__(self, o):\n",
        "        return self.x == o.x and self.y == o.y\n",
        "\n",
        "    def normalize(self):\n",
        "        norm = math.sqrt(self.x * self.x + self.y * self.y)\n",
        "        self.x /= norm\n",
        "        self.y /= norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ceeKlA7tw2IR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step-1: Setup dependencies"
      ]
    },
    {
      "metadata": {
        "id": "wyKrfUYaomnT",
        "colab_type": "code",
        "outputId": "c0ae3406-6c47-430b-ff1a-952632e47e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pygame\n",
        "import os\n",
        "\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/5e/fb7c85304ad1fd52008fd25fce97a7f59e6147ae97378afc86cf0f5d9146/pygame-1.9.4-cp36-cp36m-manylinux1_x86_64.whl (12.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.1MB 4.5MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}